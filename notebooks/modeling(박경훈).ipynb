{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# from ultralytics import RTDETR\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 전역변수 설정\n",
    "# 색상(list) (총 24개)\n",
    "COLORS = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255),\n",
    "          (0, 255, 255), (255, 128, 0), (128, 0, 255), (0, 255, 128), (255, 128, 128),\n",
    "          (128, 255, 128), (128, 128, 255), (128, 128, 0), (128, 0, 128), (0, 128, 128),\n",
    "          (192, 64, 0), (192, 192, 64), (64, 192, 192), (64, 64, 192), (192, 64, 192),\n",
    "          (64, 192, 64), (255, 192, 128), (128, 255, 192), (128, 192, 255)]\n",
    "\n",
    "# 라벨이름(dict) : area_code 및 PM_code (총 35개)\n",
    "# 주의 : 원본데이터에서 라벨 34는 제외되어있음. 즉 1부터36까지 34제외하고 35개\n",
    "LABEL_NAMES = ['인도', '횡단보도', '자전거 도로', '교차로', '중앙 차선', '안전지대',\n",
    "              '정지선', '정지선 위반 판별구역', '보행자 신호등 녹색', '보행자 신호등 적색',\n",
    "              '차량 신호등 녹색', '차량 신호등 적색', '오토바이', '오토바이_보행자도로 통행위반',\n",
    "              '오토바이_안전모 미착용', '오토바이_무단횡단', '오토바이_신호위반', '오토바이_정지선위반',\n",
    "              '오토바이_횡단보도 주행위반', '자전거', '자전거 캐리어', '자전거_보행자도로 통행위반',\n",
    "              '자전거_안전모 미착용', '자전거_무단횡단', '자전거_신호위반', '자전거_정지선위반',\n",
    "              '자전거_횡단보도 주행위반', '킥보드', '킥보드 캐리어', '킥보드_보행자도로 통행위반',\n",
    "              '킥보드_안전모 미착용', '킥보드_무단횡단', '킥보드_신호위반', '킥보드_횡단보도 주행위반',\n",
    "              '킥보드_동승자 탑승위반']\n",
    "LABEL_NAME = {str(k):v for k, v in zip(list(range(1,34)) + [35, 36], LABEL_NAMES)}\n",
    "\n",
    "# 블랙박스에서 위반사항만 검출하기위한 라벨\n",
    "LABEL_NAMES2 = ['오토바이_보행자도로 통행위반', '오토바이_안전모 미착용', '오토바이_무단횡단',\n",
    "                '오토바이_신호위반', '오토바이_정지선위반', '오토바이_횡단보도 주행위반',\n",
    "                '자전거 캐리어', '자전거_보행자도로 통행위반', '자전거_안전모 미착용',\n",
    "                '자전거_무단횡단', '자전거_신호위반', '자전거_정지선위반', '자전거_횡단보도 주행위반',\n",
    "                '킥보드 캐리어', '킥보드_보행자도로 통행위반', '킥보드_안전모 미착용', '킥보드_무단횡단',\n",
    "                '킥보드_신호위반', '킥보드_횡단보도 주행위반', '킥보드_동승자 탑승위반']\n",
    "\n",
    "# data폴더 상대경로\n",
    "FILE_PATH = 'data'\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # kernel crash 방지\n",
    "\n",
    "\n",
    "# 샘플 annot과 img확인 (파일없으면 랜덤으로)\n",
    "def random_sample(file=None):\n",
    "    \"\"\"\n",
    "    랜덤 이미지와 그에 맞는 라벨을 뽑아주는 함수\n",
    "    \"\"\"\n",
    "    if file:\n",
    "        choice = glob.glob(FILE_PATH + f'/라벨링데이터/**/{file}', recursive=True)\n",
    "        choice = choice[0]\n",
    "    else:\n",
    "        # file이 입력이 없으면 data폴더안에 아무 json 선택\n",
    "        choice = random.choice(\n",
    "            glob.glob(FILE_PATH + '/**/*.json', recursive=True)\n",
    "        )\n",
    "    # json과 그에 맞는 jpg 불러오기\n",
    "    with open(choice, 'r') as f:\n",
    "        annot = json.load(f)\n",
    "\n",
    "    choice = choice.replace('라벨링데이터', '원천데이터').replace('.json', '.jpg')\n",
    "    img = Image.open(choice)\n",
    "\n",
    "    return annot, img\n",
    "\n",
    "\n",
    "# annotation 포함된 이미지 확인\n",
    "def pic_with_annotation(annot, img):\n",
    "    \"\"\"\n",
    "    라벨(Seg, bbox)을 이미지에 올려주는 함수\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(img, 'RGBA')\n",
    "    font = ImageFont.truetype(\"./batang.ttc\", 30)\n",
    "\n",
    "    # polygon은 1부터 12까지 12종류의 고유한 area_code를 가진다.\n",
    "    # 그에 맞는 고유 색상 매핑\n",
    "    poly_color = {str(k):(r, g, b, 70) for k, (r, g, b) in \\\n",
    "                  zip(range(1,13), COLORS)}\n",
    "    # 위반코드는 13부터 36까지 (34제외하고) 23종류의 고유한 PM_code를 가진다.\n",
    "    # 그에 맞는 색상 매핑\n",
    "    box_color = {str(k):(r, g, b, 255) for k, (r, g, b) in \\\n",
    "                  zip(range(13, 37), COLORS)}\n",
    "\n",
    "    # polygon 그리기\n",
    "    for seg in annot['annotations']['environment']:\n",
    "        area_code = seg['area_code']\n",
    "        # 원본 좌표가 y,x 로 되어있으므로 뒤집기\n",
    "        points = [(x,y) for [y, x] in seg['points']]\n",
    "\n",
    "        draw.polygon(points, fill=poly_color[area_code])\n",
    "    \n",
    "    # bbox 그리기\n",
    "    for box in annot['annotations']['PM']:\n",
    "        PM_code = box['PM_code']\n",
    "        # points 양식은 [left, top, width, height] 이다.\n",
    "        # PIL 양식은 [x0, y0, x1, y1] 이므로 이에 맞게 변환\n",
    "        # 이때 x0, y0 : 왼쪽상단, x1, y1 : 오른쪽하단\n",
    "        left, top, width, height = box['points']\n",
    "        points = [left, top, left+width, top+height]\n",
    "\n",
    "        draw.rectangle(points, outline=box_color[PM_code], width=3)\n",
    "\n",
    "        text = LABEL_NAME[PM_code]\n",
    "        tbbox = draw.textbbox([points[0], points[1]-31], text, font=font)\n",
    "        draw.rectangle(tbbox, fill=box_color[PM_code])\n",
    "        draw.text([points[0], points[1]-31], text, font=font, fill='black')\n",
    "\n",
    "\n",
    "# yolo format의 Groud Truth 확인\n",
    "def yolo_gt(file=None):\n",
    "    if not file:\n",
    "        file = random.choice(glob.glob('data/coco/test/*.txt'))\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        annot = f.read()\n",
    "\n",
    "    img = Image.open(file.replace('.txt', '.jpg'))\n",
    "    draw = ImageDraw.Draw(img, 'RGB')\n",
    "\n",
    "    for line in annot.split('\\n'):\n",
    "        width = img.size[0]\n",
    "        height = img.size[1]\n",
    "        bbox = list(map(float, line.split(' ')))[1:]\n",
    "        x0 = (bbox[0] - bbox[2]/2) * width\n",
    "        y0 = (bbox[1] - bbox[3]/2) * height\n",
    "        x1 = (bbox[0] + bbox[2]/2) * width\n",
    "        y1 = (bbox[1] + bbox[3]/2) * height\n",
    "        draw.rectangle([x0,y0,x1,y1], outline='red', width=3)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO 형식에서 YOLO에 맞는 형태로 변환\n",
    "# YOLOv5 라벨형식 : class_label, x_mid(0~1), y_mid(0~1), width(0~1), height(0~1) (txt파일)\n",
    "# 그림 사이즈는 (1920*1080), (3072*1728) 로 두개이다.\n",
    "def coco_to_yolo(file, mode='train'):\n",
    "    \"\"\"\n",
    "    coco annotation을 YOLOv5 format으로 변경해누즌 함수\n",
    "    Bounding Box와 라벨만 변환\n",
    "    \"\"\"\n",
    "    # json dict가 아니라 경로로 되있으면 dict로 변환\n",
    "    if isinstance(file, str):\n",
    "        with open(file, 'r') as f:\n",
    "            file = json.load(f)\n",
    "\n",
    "\n",
    "    print(f'{mode} annotation 변환')\n",
    "    # image_id 별로 annotation추가 (string형식)\n",
    "    yolo_annots = {}\n",
    "    for annot in tqdm.tqdm(file['annotations'], desc='annot처리중'):\n",
    "        img_id = annot['image_id']\n",
    "        # YOLO의 라벨은 0부터 시작한다! COCO는 1부터 시작하므로 1빼기\n",
    "        label = annot['category_id'] - 1  \n",
    "        left = annot['bbox'][0]\n",
    "        top = annot['bbox'][1]\n",
    "        width = annot['bbox'][2]\n",
    "        height = annot['bbox'][3]\n",
    "\n",
    "        x = (left + width/2)\n",
    "        y = (top + height/2)\n",
    "\n",
    "        width = width\n",
    "        height = height\n",
    "\n",
    "        yolo_annots.setdefault(img_id, '')\n",
    "        yolo_annots[img_id] += f'{label} {x:.5f} {y:.5f} {width:.5f} {height:.5f}\\n'\n",
    "\n",
    "    # image 별로 annotation을 txt파일로 저장\n",
    "    for image in tqdm.tqdm(file['images'], desc='.txt로 저장중'):\n",
    "        file_name = image['file_name'].replace('.jpg', '.txt')\n",
    "        img_width = image['width']\n",
    "        img_height = image['height']\n",
    "        img_id = image['id']\n",
    "\n",
    "        # 이미지 크기로 bbox정규화\n",
    "        reg_annots = ''\n",
    "        for line in yolo_annots[img_id][:-1].split('\\n'):\n",
    "            ls = line.split(' ')\n",
    "            ls[1] = float(ls[1]) / img_width\n",
    "            ls[2] = float(ls[2]) / img_height\n",
    "            ls[3] = float(ls[3]) / img_width\n",
    "            ls[4] = float(ls[4]) / img_height\n",
    "\n",
    "            # 이상하면 그림 불러와서 사이즈 재확인\n",
    "            if ls[1] > 1 or ls[2] > 1 or ls[3] > 1 or ls[4] > 1:\n",
    "                with Image.open(os.path.join('data/coco/', mode, file_name.replace('.txt', '.jpg'))) as img:\n",
    "                    real_width, real_height = img.size\n",
    "\n",
    "                ls[1] = ls[1] * img_width / real_width\n",
    "                ls[2] = ls[2] * img_height / real_height\n",
    "                ls[3] = ls[3] * img_width / real_width\n",
    "                ls[4] = ls[4] * img_height / real_height\n",
    "                print(f'파일 {file_name} 표시 {img_width}*{img_height}\\t실제 {real_width}*{real_height}로 정규화됨')\n",
    "            # nan있으면 넘어가기\n",
    "            if any(np.isnan(i) for i in ls[1:]):\n",
    "                print(f'파일 {file_name} nan발생 {ls[1]}, {ls[2]}, {ls[3]}, {ls[4]}')\n",
    "                continue\n",
    "                \n",
    "            assert ls[1] <= 1, f'x이상 {ls[1]}, width {img_width} img_id {img_id} file name {file_name}'\n",
    "            assert ls[2] <= 1, f'y이상 {ls[2]}, height {img_height} img_id {img_id} file name {file_name}'\n",
    "            assert ls[3] <= 1, f'w이상 {ls[3]}, width {img_width} img_id {img_id} file name {file_name}'\n",
    "            assert ls[4] <= 1, f'h이상 {ls[4]}, height {img_height} img_id {img_id} file name {file_name}'\n",
    "            reg_annots += f'{ls[0]} {ls[1]:.5f} {ls[2]:.5f} {ls[3]:.5f} {ls[4]:.5f}\\n'\n",
    "        reg_annots = reg_annots[:-1]\n",
    "\n",
    "        # 이미지와 같은 디렉토리에 txt저장\n",
    "        with open(os.path.join(FILE_PATH, 'coco', mode, file_name), 'w') as f:\n",
    "            f.write(reg_annots)\n",
    "        \n",
    "    return \n",
    "\n",
    "# 특정경로의 txt 모두 제거\n",
    "def txt_remove(path): \n",
    "    for file in glob.glob(os.path.join(path, '*.txt')):\n",
    "        os.remove(file)\n",
    "\n",
    "# coco_to_yolo('data/coco/coco_annotations/test_annotations.json', mode='test')\n",
    "# coco_to_yolo('data/coco/coco_annotations/valid_annotations.json', mode='val')\n",
    "# coco_to_yolo('data/coco/coco_annotations/train_annotations.json', mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 블랙박스용으로, 훈련용 블랙박스데이터 11.5GB, 검증용 1.3GB 변환\n",
    "\n",
    "def to_YOLO_blackbox(mode='train'):\n",
    "    # 블랙박스 데이터 폴더 생성\n",
    "    os.makedirs(FILE_PATH + '/blackbox_yolo', exist_ok=True)\n",
    "    os.makedirs(FILE_PATH + '/blackbox_yolo/val', exist_ok=True)\n",
    "    os.makedirs(FILE_PATH + '/blackbox_yolo/train', exist_ok=True)\n",
    "\n",
    "    # x-labeller포맷변환\n",
    "    path = '/블랙박스/라벨링데이터/**/*.json' if mode=='train' else '/라벨링데이터/**/*_B_*.json'\n",
    "    jsons = glob.glob(FILE_PATH + path, recursive=True)\n",
    "    yolo_label_dict = dict(zip([i for i in range(13, 37) if i not in [13, 20, 28, 34]], range(len(LABEL_NAMES2))))\n",
    "\n",
    "    for file in tqdm.tqdm(jsons, desc=f'블랙박스 {mode} 변환중'):\n",
    "        with open(file, 'r') as f:\n",
    "            annot = json.load(f)\n",
    "\n",
    "        jpg_file = file.replace('라벨링데이터', '원천데이터').replace('.json', '.jpg')\n",
    "        jpg = Image.open(jpg_file)\n",
    "        img_width, img_height = jpg.size\n",
    "\n",
    "        # bbox와 카테고리 변환\n",
    "        string = ''\n",
    "        \n",
    "        for seg in annot['annotations']['PM']:\n",
    "            # 오토바이(13), 자전거(20), 킥보드(28)는 위반사항이 아니므로 넘어가기\n",
    "            # 이후 14~36까지 되있는 라벨을 0~19까지로 변환\n",
    "            if (label:=int(seg['PM_code'])) in [13, 20, 28]:\n",
    "                continue\n",
    "            else:\n",
    "                label = yolo_label_dict[label]\n",
    "\n",
    "            left, top, width, height = seg['points']\n",
    "            # 0~1 사이로 정규화\n",
    "            x = (left + width/2) / img_width\n",
    "            y = (top + height/2) / img_height\n",
    "            width = width / img_width\n",
    "            height = height / img_height\n",
    "\n",
    "            string += f'{label} {x:.5f} {y:.5f} {width:.5f} {height:.5f}\\n'\n",
    "        string = string[:-1]\n",
    "\n",
    "        # blackbox_yolo/train에 img옮기고 txt추가\n",
    "        shutil.copy(jpg_file, FILE_PATH + f'/blackbox_yolo/{mode}/{os.path.basename(jpg_file)}')\n",
    "        # 있는라벨들을 제거했기때문에, bbox가 없는 이미지들이 있다.\n",
    "        # 해당이미지들은 배경이미지로 학습되도록 annotation파일 만들지 않기\n",
    "        if bool(string) is True:\n",
    "            with open(FILE_PATH + f'/blackbox_yolo/{mode}/' + os.path.basename(file).replace('.json', '.txt'), 'w') as f:\n",
    "                f.write(string)\n",
    "    \n",
    "\n",
    "# to_YOLO_blackbox(mode='val')\n",
    "# to_YOLO_blackbox(mode='train')\n",
    "# train 23187개\n",
    "# val 2732개\n",
    "# 이중에 txt로 annotation이 있는게 절반정도.. 나머진 위반사항 없이 자전거, 오토바이, 킥보드만 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 데이터 비율확인\n",
    "\n",
    "# train 39496\n",
    "# valid 11284\n",
    "# test 5643\n",
    "\n",
    "files = glob.glob('data/라벨링데이터/**/*.json', recursive=True)\n",
    "n_all = len(files)\n",
    "n_cctv = len([1 for file in files if bool(re.search(r'_C_', file))])\n",
    "n_black = len([1 for file in files if bool(re.search(r'_B_', file))])\n",
    "n_day = len([1 for file in files if bool(re.search(r'_D_', file))])\n",
    "n_night = len([1 for file in files if bool(re.search(r'_N_', file))])\n",
    "n_fine = len([1 for file in files if bool(re.search(r'_F_', file))])\n",
    "n_rain = len([1 for file in files if bool(re.search(r'_R_', file))])\n",
    "\n",
    "print(f'훈련\\t: 39496')\n",
    "print(f'검증\\t: 11284')\n",
    "print(f'테스트\\t: 5643')\n",
    "print(f'총합\\t: {39496+11284+5643}\\n')\n",
    "print(f\"CCTV\\t: {n_cctv}\\t\\t비율 : {n_cctv/n_all:.3f}\")\n",
    "print(f\"블랙박스\\t: {n_black}\\t\\t비율 : {n_black/n_all:.3f}\")\n",
    "print(f\"주간\\t: {n_day}\\t\\t비율 : {n_day/n_all:.3f}\")\n",
    "print(f\"야간\\t: {n_night}\\t\\t비율 : {n_night/n_all:.3f}\")\n",
    "print(f\"맑음\\t: {n_fine}\\t\\t비율 : {n_fine/n_all:.3f}\")\n",
    "print(f\"우천\\t: {n_rain}\\t\\t비율 : {n_rain/n_all:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.suptitle('데이터세트 비율', fontsize=20)\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.pie([39496, 11284, 5643],labels=['훈련', '검증','테스트'], autopct='%.1f%%',\n",
    "        shadow=True, explode=[0, 0.1, 0.1])\n",
    "plt.title('훈련 vs 검증 vs 테스트')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.pie([n_cctv, n_black],labels=['CCTV', '블랙박스'], autopct='%.1f%%',\n",
    "        shadow=True, explode=[0, 0.3])\n",
    "plt.title('CCTV vs 블랙박스')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.pie([n_day, n_night],labels=['주간', '야간'], autopct='%.1f%%',\n",
    "        shadow=True, explode=[0, 0.3])\n",
    "plt.title('주간 vs 야간')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.pie([n_fine, n_rain],labels=['맑음', '우천'], autopct='%.1f%%',\n",
    "        shadow=True, explode=[0, 0.3])\n",
    "plt.title('맑음 vs 우천')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv8n 훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML파일 작성\n",
    "\n",
    "yaml = f\"\"\"# YOLOv8 설정파일\n",
    "# 경로설정\n",
    "path: ../{FILE_PATH}/coco/ # root (루트이전에 기본 datasets폴더를 가정하므로 ..로 한번 나가기)\n",
    "train: train\n",
    "val: val\n",
    "test: test\n",
    "\n",
    "# 클래스 설정\n",
    "nc: 23\n",
    "names: {LABEL_NAMES[12:]}\n",
    "\"\"\"\n",
    "\n",
    "with open('yolo_train.yaml', 'w') as f:\n",
    "    f.write(yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nano모델 훈련\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # kernel crash 방지\n",
    "# 디버깅 옵션\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 모델 학습 (파라미터 3M)\n",
    "# results = model.train(data='yolo_train.yaml',\n",
    "#                       epochs=50,\n",
    "#                       batch=32,\n",
    "#                       imgsz=640,\n",
    "#                       close_mosaic=0,\n",
    "#                       workers=4,\n",
    "#                       save_period=5)\n",
    "\n",
    "# 사이트 패키지 설치장소 Lib\\site-packages\\ultralytics\\utils\\plotting.py\n",
    "# 에 plt.rcParams['font.family'] = 'Gulim' 적용해야 한글라벨 정상출력\n",
    "# close_mosaic=0 으로 설정해서 mosaic augumentation 적용해제 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련된 모델 불러와서 검증\n",
    "model = YOLO('YOLOv8/train/weights/best.pt')\n",
    "\n",
    "# metrics = model.val(data='yolo_train.yaml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mAP@50-95   : {metrics.box.map:.3f}')\n",
    "print(f'mAP@50      : {metrics.box.map50:.3f}')\n",
    "print(f'mAP@75      : {metrics.box.map75:.3f}')\n",
    "print(f'inferece speed : {metrics.speed[\"inference\"]:.3f}ms')\n",
    "\n",
    "cls_maps = [metrics.maps[i] for i in range(23)][::-1]\n",
    "cls_names = [metrics.names[i] for i in range(23)][::-1]\n",
    "c = ['C1' if i==max(cls_maps) else 'C2' if i==min(cls_maps) else 'C0' for i in cls_maps]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(cls_names, cls_maps, color=c)\n",
    "plt.title('YOLOv8n 29Epochs 클래스별 mAP@50-95', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# 오토바이_정지선위반이 가장 정확도가 낮다. (~0.72)\n",
    "# 오토바이_횡단보도 주행위반이 가장 정확도가 낮다. (~0.14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT-DETR 훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML파일 작성\n",
    "\n",
    "yaml = f\"\"\"# RT-DETR-l 설정파일\n",
    "# 경로설정\n",
    "path: ../{FILE_PATH}/blackbox_yolo/ # root (루트이전에 기본 datasets폴더를 가정하므로 ..로 한번 나가기)\n",
    "train: train\n",
    "val: val\n",
    "test: test\n",
    "\n",
    "# 클래스 설정\n",
    "nc: 23\n",
    "names: {LABEL_NAMES[12:]}\n",
    "\"\"\"\n",
    "\n",
    "with open('rtdetr_train.yaml', 'w') as f:\n",
    "    f.write(yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 훈련시간이 길것으로 예상..\n",
    "\n",
    "model2 = RTDETR('rtdetr-l.pt')\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # kernel crash 방지\n",
    "# 디버깅 옵션\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 모델 학습 (파라미터 32M)\n",
    "# results = model2.train(data='rtdetr_train.yaml',\n",
    "#                        epochs=50,\n",
    "#                        batch=4,\n",
    "#                        imgsz=640,\n",
    "#                        close_mosaic=2,\n",
    "#                        workers=4,\n",
    "#                        project='RTDETR',\n",
    "#                        name='train',\n",
    "#                        freeze=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩에서 50 epoch 학습시킨 모델 불러와서 평가\n",
    "model2 = RTDETR('RTDETR/train/weights/best.pt')\n",
    "# metrics = model2.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mAP@50-95   : {metrics.box.map:.3f}')\n",
    "print(f'mAP@50      : {metrics.box.map50:.3f}')\n",
    "print(f'mAP@75      : {metrics.box.map75:.3f}')\n",
    "print(f'inferece speed : {metrics.speed[\"inference\"]:.3f}ms')\n",
    "\n",
    "cls_maps = [metrics.maps[i] for i in range(20)][::-1]\n",
    "cls_names = [metrics.names[i] for i in range(20)][::-1]\n",
    "c = ['C1' if i==max(cls_maps) else 'C2' if i==min(cls_maps) else 'C0' for i in cls_maps]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(cls_names, cls_maps, color=c)\n",
    "plt.title('RT-DETR-l 50Epochs 클래스별 mAP@50-95', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# 오토바이_정지선위반이 가장 정확도가 낮다. (~0.9)\n",
    "# 오토바이_횡단보도 주행위반이 가장 정확도가 낮다. (~0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv8m 훈련 및 평가\n",
    "* 기존 RT-DETR과 YOLOv8n의 트래킹성능이 별로다.\n",
    "    1. 클래스의 형상이 비슷해서, 하나의 객체에 여러 클래스가 잡히는경우있음\n",
    "    2. 클래스 정확도는 낮고, bbox 위치정확도는 높음.  \n",
    ">\n",
    "* YOLOv8m 으로 학습 (파라미터 25M)\n",
    "* 추론시 agnostic-NMS를 사용한다. 또, cls_loss를 높이고 box_loss를 낮추어 학습\n",
    "* RT-DETR의 데이터 적용사항 롤백. (다시 23개 클래스사용,  블박, CCTV 전부 포함하여 학습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8m YAML파일 작성\n",
    "FILE_PATH = '/data/coco'\n",
    "\n",
    "data_yaml = f\"\"\"# YOLOv8m 설정파일\n",
    "# 경로설정\n",
    "path: ..{FILE_PATH} # root (루트이전에 기본 datasets폴더를 가정하므로 ..로 한번 나가기)\n",
    "train: train\n",
    "val: val\n",
    "test: test\n",
    "\n",
    "# 클래스 설정\n",
    "nc: 23\n",
    "names: {LABEL_NAMES[12:]}\n",
    "\"\"\"\n",
    "\n",
    "train_yaml = f\"\"\"# 학습 및 추론환경 설정\n",
    "# 현재 데이터는 사실상 오토바이, 자전거, 킥보드 3개의 형상밖에 없다.\n",
    "# 비슷한 형상에, 여러가지 위반사항이 존재한다.\n",
    "# 따라서 형상 위치를 찾는것(localization)은 쉽지만, 어떤 위반인지 판별(classification)이 힘들다.\n",
    "# 위반사항의 불균형이 있으며, 이에따라 AP의 차이가 있는편이다.\n",
    "\n",
    "# val & pred  ------------------------------------------------\n",
    "agnostic_nms: True  # 추론시 클래스 상관없이 NMS\n",
    "iou: 0.8 # default 0.7. agnostic-NMS인만큼 IoU 기준을 높임 (근처의 다른 위반행위 삭제 방지)\n",
    "max_det: 50 # default 300. 일반적으로 도로에 오토바이, 자전거, 킥보드수가 많진 않으므로 최대 50개 탐지\n",
    "half: True # 기본적으로 반정밀도 사용\n",
    "\n",
    "# train ------------------------------------------------\n",
    "close_mosaic: 20 # 학습종료 20epoch 전부터 모자이크 증강 해제\n",
    "box: 5.0 # default 7.5. bbox좌표오차 패널티 완화\n",
    "cls: 3.0 # default 0.5. 클래스 오분류 패널티 강화\n",
    "dfl: 2.0 # default 1.5. 클래스 불균형을 해결하기위해 살짝 증가\n",
    "label_smoothing: 0.1 # 예측확률 신뢰도 증가 및 일반화성능 증가를 위해 라벨 스무딩\n",
    "cache: True\n",
    "\n",
    "# augmentation ------------------------------------------------\n",
    "hsv_s: 0.2  # default 0.7(frac). 채도 변경비율 낮추기\n",
    "hsv_v: 0.2  # default 0.4(frac). 명도 변경비율 낮추기\n",
    "scale: 0.1  # default 0.5(gain). 이미지 스케일 변경 범위 낮추기\n",
    "mosaic: 0.5 # default 1(prob). 모자이크 확률 낮추기\n",
    "\"\"\"\n",
    "\n",
    "with open('yolov8m_data.yaml', 'w') as f:\n",
    "    f.write(data_yaml)\n",
    "    print('data yaml 추가완료')\n",
    "\n",
    "with open('yolov8m_train.yaml', 'w') as f:\n",
    "    f.write(train_yaml)\n",
    "    print('train yaml 추가완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8m 모델 훈련 (코랩 A100으로 진행)\n",
    "model3 = YOLO('yolov8m.pt')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # kernel crash 방지\n",
    "# results = model3.train(data='yolov8m_data.yaml',\n",
    "#                        cfg='yolov8m_train.yaml',\n",
    "#                        epochs=100,\n",
    "#                        patience=10,\n",
    "#                        batch=-1, # 자동배치\n",
    "#                        imgsz=640,\n",
    "#                        project='drive/MyDrive/YOLOv8m',\n",
    "#                        name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련된 모델 불러오기\n",
    "model3 = YOLO('YOLOv8m/train/weights/last.pt')\n",
    "metrics = model3.val(project='YOLOv8m', name='val2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(f'mAP@50-95   : {metrics.box.map:.3f}')\n",
    "print(f'mAP@50      : {metrics.box.map50:.3f}')\n",
    "print(f'mAP@75      : {metrics.box.map75:.3f}')\n",
    "print(f'inferece speed : {metrics.speed[\"inference\"]:.3f}ms')\n",
    "\n",
    "cls_maps = [metrics.maps[i] for i in range(20)][::-1]\n",
    "cls_names = [metrics.names[i] for i in range(20)][::-1]\n",
    "c = ['C1' if i==max(cls_maps) else 'C2' if i==min(cls_maps) else 'C0' for i in cls_maps]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(cls_names, cls_maps, color=c)\n",
    "plt.title('YOLOv8m 78Epochs 클래스별 mAP@50-95', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가로 훈련한 모델 평가\n",
    "# 모자이크 증강없애고, 스케일 증강 확대, AdamW 옵티마이저로 변경, 배치사이즈 16으로 줄여서 40Epoch 추가로 학습\n",
    "\n",
    "# 훈련된 모델 불러오기\n",
    "model4 = YOLO('YOLOv8m/second_train/weights/last.pt')\n",
    "# metrics = model4.val(project='YOLOv8m', name='train2_val', verbose=False)\n",
    "\n",
    "print(f'mAP@50-95   : {metrics.box.map:.3f}')\n",
    "print(f'mAP@50      : {metrics.box.map50:.3f}')\n",
    "print(f'mAP@75      : {metrics.box.map75:.3f}')\n",
    "print(f'inferece speed : {metrics.speed[\"inference\"]:.3f}ms')\n",
    "\n",
    "cls_maps = [metrics.maps[i] for i in range(20)][::-1]\n",
    "cls_names = [metrics.names[i] for i in range(20)][::-1]\n",
    "c = ['C1' if i==max(cls_maps) else 'C2' if i==min(cls_maps) else 'C0' for i in cls_maps]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(cls_names, cls_maps, color=c)\n",
    "plt.title('YOLOv8m 118Epochs 클래스별 mAP@50-95', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Faster_RCNN (5epoch)', 'RTMDet (10epoch)', 'YOLOv3mobile (3epoch)', 'RT-DETR-l (50epoch)',\n",
    "               'YOLOv8n (50epoch)', 'YOLOv8s (50epoch)', 'YOLOv8m (78epoch)']\n",
    "model_mAP5095 = [0.35, 0.23, 0.24, 0.596, 0.57, 0.62, 0.61]\n",
    "\n",
    "plt.barh(model_names, model_mAP5095,\n",
    "         color=['C1' if i==max(model_mAP5095) else 'C0' for i in model_mAP5095])\n",
    "plt.title('val mAP50-95 of 7 models', fontsize=20)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비디오 트래킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = YOLO('YOLOv8m/second_train/weights/last.pt')\n",
    "                                  \n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('data/example2.mkv')\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('example.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 20, (video_width, video_height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model4.predict(frame,\n",
    "                                 conf=0.1,\n",
    "                                 iou=0.7,\n",
    "                                 agnostic_nms=True)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.namedWindow('YOLOv8m Tracking', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"YOLOv8m Tracking\", annotated_frame)\n",
    "\n",
    "        out.write(annotated_frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = YOLO('YOLOv8m/second_train/weights/last.pt')\n",
    "\n",
    "# 비디오 열기\n",
    "video_path = \"data/example3.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter('example4_track.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 30, (video_width, video_height))\n",
    "\n",
    "# 트래킹 히스토리 딕셔너리 선언\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if success:\n",
    "        # 프레임별로 트래킹\n",
    "        results = model4.track(frame, persist=True, tracker='botsort.yaml', imgsz=640, agnostic_nms=True)\n",
    "\n",
    "        # bbox와 id 획득\n",
    "        boxes = results[0].boxes.xywh.cpu()\n",
    "        try:\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        except:\n",
    "            track_ids = [None] * len(boxes)\n",
    "\n",
    "        # bbox 및 id추론 표시된 프레임\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # bbox 중앙에 흰점으로 트래킹 결과 표시\n",
    "        for box, track_id in zip(boxes, track_ids):\n",
    "            x, y, w, h = box\n",
    "            track = track_history[track_id]\n",
    "            track.append((float(x), float(y)))\n",
    "            if len(track) > 30:  # 30프레임 이상 유지시 처음 트랙히스토리 삭제\n",
    "                track.pop(0)\n",
    "\n",
    "            # 흰점 표시\n",
    "            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        # 추론결과 표시 (저장만할시 주석처리)\n",
    "        cv2.namedWindow('YOLOv8 Tracking', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "\n",
    "        # 추론결과 저장\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # q 누르면 강제종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# 비디오 리더와 라이터 객체, 창 닫기\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 ONNX export 및 ONNX 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = YOLO('YOLOv8m/second_train/weights/last.pt')\n",
    "# opset 17 Export\n",
    "# model4.export(format='ONNX', imgsz=640, simplify=True) # CPU용 Export\n",
    "# model4.export(format='ONNX', imgsz=640, half=True) # GPU용 Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_img(file=None):\n",
    "    if not file:\n",
    "        file = random.choice(glob.glob('data/coco/test/*.jpg'))\n",
    "    array = cv2.imread(file)\n",
    "    # BGR 형태로 반환\n",
    "    return array\n",
    "\n",
    "test_img = choose_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2.dnn\n",
    "import PIL\n",
    "import time\n",
    "\n",
    "onnx_model = cv2.dnn.readNetFromONNX('YOLOv8m/second_train/weights/YOLOv8m.onnx')\n",
    "\n",
    "def inference(model:cv2.dnn.Net, img, conf=0.25, nms_th=0.8):\n",
    "    \"\"\"\n",
    "    ONNX 모델 받아서 추론\n",
    "    \n",
    "    returns:\n",
    "        result : bbox표기된 이미지 (width, height, RGB)\n",
    "        infer_time : 추론시간 (ms)\n",
    "    \"\"\"\n",
    "    CLASS_NAMES = LABEL_NAMES[12:]\n",
    "    [height, width, _] = img.shape\n",
    "    length = max((height, width))\n",
    "    resized_img = np.zeros((length, length, 3), np.uint8)\n",
    "    resized_img[0:height, 0:width] = img\n",
    "    scale = length / 640\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(resized_img, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
    "    model.setInput(blob)\n",
    "\n",
    "    # 출력 형태 (batch, 27, 8400)\n",
    "    # 여기서 27은 차례대로 bbox좌표4개(x,y,w,h) + 클래스 23개\n",
    "    # bbox의 숫자 8400개\n",
    "    t1 = time.time()\n",
    "    outputs = model.forward()\n",
    "    t2 = time.time()\n",
    "    infer_time = round((t2-t1)*1000, 2)\n",
    "    # 출력형태 (8400, 27)로 변환 후 agnostic NMS 진행\n",
    "    outputs = outputs[0].transpose()\n",
    "\n",
    "    boxes = [] \n",
    "    cls_ids = []\n",
    "    scores = []\n",
    "    # 기준신뢰도 이상의 box만 추출\n",
    "    for row in outputs:\n",
    "        cls_score = row[4:]\n",
    "        x, y, w, h = row[:4]\n",
    "        # 최대 최소 및 최대 index 구하기\n",
    "        minScore, maxScore, (_, minClassLoc), (_, maxClassIndex) = cv2.minMaxLoc(cls_score)\n",
    "        if maxScore <= conf: # 기준 신뢰도 이하면 버리고 다음 row\n",
    "            continue\n",
    "        # left, top, width, height로 변환 (NMSBoxes input format)\n",
    "        x1, y1= x-(0.5*w), y-(0.5*h)\n",
    "        boxes.append([x1,y1,w,h])\n",
    "        scores.append(maxScore)\n",
    "        cls_ids.append(maxClassIndex)\n",
    "    \n",
    "    # agnostic NMS (선택된 bbox index list출력)\n",
    "    nms_indices = cv2.dnn.NMSBoxes(boxes, scores, conf, nms_th)\n",
    "\n",
    "    # 선택된 bbox 그리기 (cv2는 한글폰트 적용 x => PIL 사용)\n",
    "    img = PIL.Image.fromarray(img[..., ::-1])\n",
    "    font = PIL.ImageFont.truetype(\"./batang.ttc\", 30)\n",
    "    draw = PIL.ImageDraw.Draw(img, 'RGB')\n",
    "    for idx in nms_indices:\n",
    "        cls_name = CLASS_NAMES[cls_ids[idx]]\n",
    "        score = scores[idx]\n",
    "        x1, y1, w, h = np.array(boxes[idx]) * scale # 640*640에 원본스케일 곱하기\n",
    "        color = COLORS[cls_ids[idx]]\n",
    "\n",
    "        draw.rectangle((x1,y1,x1+w,y1+h), outline=color, width=5)\n",
    "        tbbox = draw.textbbox([x1, y1-30], f'{score:.2f} '+cls_name, font=font)\n",
    "        draw.rectangle(tbbox, fill=color)\n",
    "        draw.text([x1, y1-30], f'{score:.2f} '+cls_name, font=font, fill='black')\n",
    "\n",
    "    # 640*640*RGB 행렬 반환\n",
    "    result = np.asarray(img)\n",
    "    return result, infer_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnxrt_model = onnxruntime.InferenceSession('YOLOv8m.onnx')\n",
    "\n",
    "\n",
    "def onnxrt_inference(model, img, conf=0.25, nms_th=0.8):\n",
    "    \"\"\"\n",
    "    ONNX 모델 받아서 추론 (ONNX runtime 버전)\n",
    "    \n",
    "    returns:\n",
    "        result : bbox표기된 이미지 (width, height, RGB)\n",
    "        infer_time : 추론시간 (ms)\n",
    "    \"\"\"\n",
    "    CLASS_NAMES = LABEL_NAMES[12:]\n",
    "    [height, width, _] = img.shape\n",
    "    length = max((height, width))\n",
    "    resized_img = np.zeros((length, length, 3), np.float32)\n",
    "    resized_img[0:height, 0:width] = img / 255\n",
    "    resized_img = cv2.resize(resized_img, (640,640))[np.newaxis,...].transpose(0, 3, 1, 2)\n",
    "    scale = length / 640\n",
    "\n",
    "    input_name = model.get_inputs()[0].name\n",
    "\n",
    "    # 출력 형태 (batch, 27, 8400)\n",
    "    # 여기서 27은 차례대로 bbox좌표4개(x,y,w,h) + 클래스 23개\n",
    "    # bbox의 숫자 8400개\n",
    "    t1 = time.time()\n",
    "    outputs = model.run(None, {input_name:resized_img})\n",
    "    t2 = time.time()\n",
    "    infer_time = round((t2-t1)*1000, 2)\n",
    "    # 출력형태 (8400, 27)로 변환 후 agnostic NMS 진행\n",
    "    outputs = outputs[0].transpose().squeeze()\n",
    "    \n",
    "    boxes = [] \n",
    "    cls_ids = []\n",
    "    scores = []\n",
    "    # 기준신뢰도 이상의 box만 추출\n",
    "    for row in outputs:\n",
    "        cls_score = row[4:]\n",
    "        x, y, w, h = row[:4]\n",
    "        # 최대 최소 및 최대 index 구하기\n",
    "        minScore, maxScore, (_, minClassLoc), (_, maxClassIndex) = cv2.minMaxLoc(cls_score)\n",
    "        if maxScore <= conf: # 기준 신뢰도 이하면 버리고 다음 row\n",
    "            continue\n",
    "        # left, top, width, height로 변환 (NMSBoxes input format)\n",
    "        x1, y1= x-(0.5*w), y-(0.5*h)\n",
    "        boxes.append([x1,y1,w,h])\n",
    "        scores.append(maxScore)\n",
    "        cls_ids.append(maxClassIndex)\n",
    "    \n",
    "    # agnostic NMS (선택된 bbox index list출력)\n",
    "    nms_indices = cv2.dnn.NMSBoxes(boxes, scores, conf, nms_th)\n",
    "\n",
    "    # 선택된 bbox 그리기 (cv2는 한글폰트 적용 x => PIL 사용)\n",
    "    img = PIL.Image.fromarray(img[..., ::-1])\n",
    "    font = PIL.ImageFont.truetype(\"./batang.ttc\", 30)\n",
    "    draw = PIL.ImageDraw.Draw(img, 'RGB')\n",
    "    for idx in nms_indices:\n",
    "        cls_name = CLASS_NAMES[cls_ids[idx]]\n",
    "        score = scores[idx]\n",
    "        x1, y1, w, h = np.array(boxes[idx]) * scale # 640*640에 원본스케일 곱하기\n",
    "        color = COLORS[cls_ids[idx]]\n",
    "\n",
    "        draw.rectangle((x1,y1,x1+w,y1+h), outline=color, width=5)\n",
    "        tbbox = draw.textbbox([x1, y1-30], f'{score:.2f} '+cls_name, font=font)\n",
    "        draw.rectangle(tbbox, fill=color)\n",
    "        draw.text([x1, y1-30], f'{score:.2f} '+cls_name, font=font, fill='black')\n",
    "\n",
    "    # 640*640*RGB 행렬 반환\n",
    "    result = np.asarray(img)\n",
    "    return result, infer_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = choose_img()\n",
    "result, infer_time = onnxrt_inference(onnxrt_model, test_img, conf=0.5)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(result)\n",
    "plt.title(f'cv2.dnn 추론시간 {infer_time}ms', fontsize=15)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "result, infer_time = inference(onnx_model, test_img, conf=0.5)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(result)\n",
    "plt.title(f'ONNX runtime 추론시간 {infer_time}ms', fontsize=15)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# cv2.dnn보다 onnxruntime이 추론시간이 매우빠르다.\n",
    "# 런타임 패키지도 포함해서 서버백엔드 구성하는편이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('data/example3.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "    # 프레임 읽기\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # 성공시 프레임 추론\n",
    "    if success:\n",
    "        results, infer_time = inference(onnx_model, frame, conf=0.2)\n",
    "\n",
    "        cv2.namedWindow('YOLOv8m Tracking', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"YOLOv8m Tracking\", results[..., ::-1]) # RGB => BGR\n",
    "        print(f'추론시간 {infer_time}ms')\n",
    "\n",
    "        # 키보드 q 누르면 중간 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            cap.release()\n",
    "            break\n",
    "    \n",
    "    # 프레임 전부 읽어들이면 종료\n",
    "    else:\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
